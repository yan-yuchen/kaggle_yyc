{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', None)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Statistical Tests\nfrom scipy.stats import f_oneway\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n# Model Selection\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, classification_report\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVC\n\n# Models\nimport optuna\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.linear_model import Lasso, Ridge, LogisticRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nimport haversine as hs \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport re\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, KFold, TimeSeriesSplit\nfrom sklearn.model_selection import GroupKFold, LeaveOneGroupOut\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor, StackingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import squareform\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"reference:\n\nhttps://www.kaggle.com/code/yaaangzhou/en-playground-s3-e20-eda-modeling/notebook","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/playground-series-s3e20/sample_submission.csv')\ntrain = pd.read_csv('/kaggle/input/playground-series-s3e20/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e20/test.csv')\n\ntrain.drop(['ID_LAT_LON_YEAR_WEEK'], axis=1, inplace=True)\ntest.drop(['ID_LAT_LON_YEAR_WEEK'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.visualization","metadata":{}},{"cell_type":"code","source":"# Latitude and longitude\n\nfig = px.scatter_mapbox(train, lat=\"latitude\", lon=\"longitude\", \n                        zoom=7, height=800, width=1000,) \nfig.update_layout(mapbox_style=\"open-street-map\", title='Map')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"week\"] = (train[\"year\"] - 2019) * 53 + train[\"week_no\"]\n\ndef plot_emission(train):\n    \n    plt.figure(figsize=(15, 6))\n    sns.lineplot(data=train, x=\"week\", y=\"emission\", label=\"Emission\", alpha=0.7, color='blue')\n\n    plt.xlabel('Week')\n    plt.ylabel('Emission')\n    plt.title('Emission over time')\n\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n    \nplot_emission(train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num = train.columns.tolist()[1:-1]\n\ncorr_matrix = train[num].corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nplt.figure(figsize=(15, 12))\nsns.heatmap(corr_matrix, mask=mask, annot=False, cmap='Blues', fmt='.2f', linewidths=1, square=True, annot_kws={\"size\": 9} )\nplt.title('Correlation Matrix', fontsize=15)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.EDA","metadata":{}},{"cell_type":"code","source":"# add feature\ntrain=train[train['year']!=2020] # 删除疫情数据\n\n\ntrain['week_no_sin'] = np.sin(train['week_no']*(2*np.pi/52))\ntrain['week_no_cos'] = np.cos(train['week_no']*(2*np.pi/52))\n\ntest['week_no_sin'] = np.sin(test['week_no']*(2*np.pi/52))\ntest['week_no_cos'] = np.cos(test['week_no']*(2*np.pi/52))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 对缺失值做处理，对部分变量做scalar\n\ndef feature_preprocessing(df):\n\n    # drop features with more than 50% nan.\n    missing_ratios = df.isnull().mean()\n    columns_to_drop = missing_ratios[missing_ratios > 0.5].index\n    df = df.drop(columns_to_drop, axis=1)\n    df = df.fillna(df.mean())\n\n    # scalar\n    sc = StandardScaler()\n    for i in df.columns:\n        if i not in ['week_no', 'covid_flag','latitude','longitude','emission']:\n            df[i] = sc.fit_transform(df[i].values.reshape(-1,1))\n    return df\n\ntrain = feature_preprocessing(train)\ntest = feature_preprocessing(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emission = train.pivot(index=['latitude', 'longitude'], \n                       columns=['year', 'week_no'], values='emission')\nwith pd.option_context(\"display.max_columns\", 6, \"display.max_rows\", 6):\n    display(emission)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/ambrosm/pss3e20-eda-which-makes-sense\n# A PCA or an SVD of the 497 emission time series reveals that five components suffice to explain 100 % of the variance:\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom matplotlib.ticker import MaxNLocator\n\nsvd = TruncatedSVD(n_components=10)\nsvd.fit(emission)\n\nplt.figure(figsize=(5, 3))\nplt.plot(svd.explained_variance_ratio_.cumsum(), color='g')  # explained_variance_ratio_:所选择的每个组成部分所解释的方差百分比\nplt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.title('Singular value decomposition of the locations')\nplt.ylabel('Cumulative explained variance ratio')\nplt.xlabel('Component #')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clustering by emission mean\n# 根据地区的排放量进行聚类\ncluster_train = train.groupby(by=['latitude', 'longitude'], as_index=False)['emission'].mean()\nmodel = KMeans(n_clusters=5)\nmodel.fit(cluster_train)\ncluster_train['kmeans_group'] = model.predict(cluster_train)\n\n# check if emission is 0\ncluster_train['is_zero'] = cluster_train['emission'].apply(lambda x: 1 if x==0 else 0)\n\n# Distance to the highest emission location\n# 找到同一地区的历史最高排放量，并计算距离它的距离 \nmax_lat_lon_emission = cluster_train.loc[cluster_train['emission']==cluster_train['emission'].max(), ['latitude', 'longitude']]\ncluster_train['distance_to_max_emission'] = cluster_train.apply(\n    lambda x: hs.haversine((x['latitude'], x['longitude']), (max_lat_lon_emission['latitude'].values[0], max_lat_lon_emission['longitude'].values[0])), axis=1)\n\ntrain = train.merge(cluster_train[['latitude', 'longitude', 'kmeans_group', 'is_zero', 'distance_to_max_emission']], on=['latitude', 'longitude'])\ntest = test.merge(cluster_train[['latitude', 'longitude', 'kmeans_group', 'is_zero', 'distance_to_max_emission']], on=['latitude', 'longitude'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.train","metadata":{}},{"cell_type":"code","source":"X = train.drop('emission',axis=1)\nY = train['emission']\n\n# This data is for FS\n# 经过预处理的数据将用在特征选择\ntrain_fs = pd.concat([X,Y],axis = 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 使用同一模型XGBoost，对比不同feature下的RMSE\ndef calculateRMSE(x,y,features,model):\n    for i,val in enumerate(x.columns):\n        if val not in features:\n            x = x.drop(columns=val)\n    X_train, X_val, y_train, y_val = train_test_split(x,y,test_size=0.2, random_state=42)\n    clf = XGBRegressor().fit(X_train,y_train)\n    prediction = clf.predict(X_val)\n    loss = np.sqrt(mean_squared_error(y_val,prediction))\n    \n    print(f\"{model} has RMSE = {loss}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_feature=train.columns.values\nfeature_1 = X.drop(['longitude', 'kmeans_group', 'is_zero', 'distance_to_max_emission'],axis=1).columns\nfeature_2 = X.drop(['longitude','distance_to_max_emission'],axis=1).columns\n\ncalculateRMSE(X,Y,all_feature,'All features')\ncalculateRMSE(X,Y,feature_1,'eatures 1')\ncalculateRMSE(X,Y,feature_2,'All features 2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feat_cols = list(sfs_1.k_feature_idx_)\n# featureSFS = X.columns[feat_cols].tolist()\n\nfeatureSFS = ['latitude',\n 'longitude',\n 'year',\n 'week_no',\n 'CarbonMonoxide_sensor_azimuth_angle',\n 'Cloud_solar_azimuth_angle',\n 'week_no_sin',\n 'kmeans_group',\n 'is_zero',\n 'distance_to_max_emission']\n\nprint(\"Selected features:\", featureSFS)\ncalculateRMSE(X,Y,featureSFS,'Step Forward Selection')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selected_feats = ['kmeans_group', 'longitude', 'week_no', 'distance_to_max_emission', 'latitude', 'ratio_2020']\nselected_feats = featureSFS\n# selected_feats = featureRFE\nX = train[selected_feats]\nY = Y\n\ntest = test[selected_feats]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cv_scores, xgb_preds = list(), list()\nlgbm_cv_scores, lgbm_preds = list(), list()\n# ens_cv_scores, ens_preds = list(), list()\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nfor i, (train_ix, test_ix) in enumerate(kf.split(X)):\n    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n    Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]\n    \n    print('---------------------------------------------------------------')\n    \n    ## XGBoost\n    xgb_md = XGBRegressor().fit(X_train, Y_train)\n    xgb_pred = xgb_md.predict(X_test)   \n    xgb_score_fold = np.sqrt(mean_squared_error(Y_test, xgb_pred))\n    print('Fold', i+1, '==> XGBoost oof RMSE score is ==>', xgb_score_fold)\n    xgb_cv_scores.append(xgb_score_fold)\n    \n    xgb_pred_test = xgb_md.predict(test)\n    xgb_preds.append(xgb_pred_test)\n    \n    ## LGBM\n    lgbm_md = LGBMRegressor(n_estimators = 1000,\n                           max_depth = 15,\n                           learning_rate = 0.01,\n                           num_leaves = 105,\n                           reg_alpha = 0.1, #0.3\n                           reg_lambda = 0.1,\n                           subsample = 0.7,\n                           colsample_bytree = 0.8,\n                           verbose=-1, # 关闭日志打印\n                           ).fit(X_train, Y_train)\n    lgbm_pred = lgbm_md.predict(X_test) \n    lgbm_score_fold = np.sqrt(mean_squared_error(Y_test, lgbm_pred))\n    print('Fold', i+1, '==> LGBM oof RMSE score is ==>', lgbm_score_fold)\n    lgbm_cv_scores.append(lgbm_score_fold)\n    \n    lgbm_pred_test = lgbm_md.predict(test)\n    lgbm_preds.append(lgbm_pred_test) \n    \nprint('---------------------------------------------------------------')\nprint('Average RMSE of XGBoost model is:', np.mean(xgb_cv_scores))\nprint('Average RMSE of LGBM model is:', np.mean(lgbm_cv_scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = sample_submission.copy()\n\nsubmission_xgb = xgb_md.predict(test)\nsubmission_lgb = lgbm_md.predict(test)\n\npre_count=(0.5*submission_xgb+0.5*submission_lgb)*1.06\nsubmission['emission'] = pre_count\n\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}